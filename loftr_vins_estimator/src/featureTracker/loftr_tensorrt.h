#ifndef LOFTR_TENSORRT_H
#define LOFTR_TENSORRT_H

#include <opencv2/opencv.hpp>
#include <vector>
#include <string>
#include <memory>

// ğŸ”§ ä½¿ç”¨ CMakeLists.txt ä¸­å®šä¹‰çš„å®
#ifdef TENSORRT_AVAILABLE
#include <NvInfer.h>
#include <NvOnnxParser.h>
#include <cuda_runtime.h>
#include <NvInferRuntime.h>
#endif

/**
 * @brief LoFTR TensorRT åç«¯å®ç°
 * æä¾›é«˜æ€§èƒ½çš„ GPU æ¨ç†æ”¯æŒ
 * 
 * ğŸ”§ ä¿®å¤è¯´æ˜ï¼š
 * - åŸºäº Python ç‰ˆæœ¬åˆ†æï¼ŒTensorRT è¾“å‡ºä¸¤ä¸ª 1200x1200 çŸ©é˜µ
 * - ä½¿ç”¨ç¬¬äºŒä¸ªè¾“å‡ºä½œä¸º confidence matrixï¼ˆä¸ Python ç‰ˆæœ¬ä¸€è‡´ï¼‰
 * - æ”¹è¿›äº†è¾“å‡ºå¤„ç†å’Œåæ ‡è½¬æ¢é€»è¾‘
 */
class LoFTR_TensorRT {
public:
    LoFTR_TensorRT();
    ~LoFTR_TensorRT();
    
    /**
     * @brief æ£€æŸ¥ TensorRT æ˜¯å¦å¯ç”¨
     * @return TensorRT æ˜¯å¦å¯ç”¨
     */
    static bool isAvailable();
    
    /**
     * @brief åˆå§‹åŒ– TensorRT å¼•æ“
     * @param model_path ONNX æ¨¡å‹æ–‡ä»¶è·¯å¾„
     * @param engine_path TensorRT å¼•æ“ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
     * @param input_width è¾“å…¥å›¾åƒå®½åº¦
     * @param input_height è¾“å…¥å›¾åƒé«˜åº¦
     * @return åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
     */
    bool initialize(const std::string& model_path, 
                   const std::string& engine_path = "",
                   int input_width = 640, 
                   int input_height = 480);
    
    /**
     * @brief æ‰§è¡Œæ¨ç†
     * @param img0 ç¬¬ä¸€å¼ å›¾åƒï¼ˆå·²é¢„å¤„ç†ï¼‰
     * @param img1 ç¬¬äºŒå¼ å›¾åƒï¼ˆå·²é¢„å¤„ç†ï¼‰
     * @param output è¾“å‡ºç»“æœï¼ˆconfidence matrixï¼Œ1200x1200ï¼‰
     * @return æ¨ç†æ˜¯å¦æˆåŠŸ
     */
    bool infer(const cv::Mat& img0, const cv::Mat& img1, std::vector<float>& output);
    
    /**
     * @brief æ£€æŸ¥æ˜¯å¦å·²åˆå§‹åŒ–
     * @return æ˜¯å¦å·²åˆå§‹åŒ–
     */
    bool isInitialized() const { return initialized_; }
    
    /**
     * @brief è·å–æ¨¡å‹è¾“å…¥å°ºå¯¸
     * @return è¾“å…¥å°ºå¯¸ (width, height)
     */
    cv::Size getInputSize() const { return cv::Size(input_width_, input_height_); }
    
    /**
     * @brief è·å–å¼•æ“ä¿¡æ¯
     * @return å¼•æ“ä¿¡æ¯å­—ç¬¦ä¸²
     */
    std::string getEngineInfo() const;

private:
    // åŸºç¡€æˆå‘˜å˜é‡
    bool initialized_;
    int input_width_;
    int input_height_;
    int batch_size_;
    int channels_;

    // ğŸ”§ ä¿®å¤ï¼šLoggerç±»å®šä¹‰ç§»åˆ°æ¡ä»¶ç¼–è¯‘å¤–ï¼Œç¡®ä¿è™šå‡½æ•°è¡¨å®Œæ•´
    class Logger : public 
#ifdef TENSORRT_AVAILABLE
        nvinfer1::ILogger
#else
        // å½“ TensorRT ä¸å¯ç”¨æ—¶æä¾›ä¸€ä¸ªç©ºçš„åŸºç±»
        struct { public: enum Severity { kINTERNAL_ERROR, kERROR, kWARNING, kINFO, kVERBOSE }; }
#endif
    {
    public:
#ifdef TENSORRT_AVAILABLE
        void log(Severity severity, const char* msg) noexcept override;
#else
        void log(int severity, const char* msg) {} // ç©ºå®ç°
#endif
    };

#ifdef TENSORRT_AVAILABLE
    /**
     * @brief TensorRT ä¸“ç”¨æ–¹æ³•
     */
    bool buildEngineFromONNX(const std::string& onnx_path, const std::string& engine_path);
    bool loadEngineFromFile(const std::string& engine_path);
    bool saveEngineToFile(const std::string& engine_path);
    bool prepareBuffers();
    void copyInputToDevice(const cv::Mat& img0, const cv::Mat& img1);
    
    /**
     * @brief ä»è®¾å¤‡å¤åˆ¶è¾“å‡ºæ•°æ®
     * ğŸ”§ ä¿®å¤ï¼šåŸºäº Python ç‰ˆæœ¬ï¼Œä½¿ç”¨ç¬¬äºŒä¸ªè¾“å‡ºçŸ©é˜µ
     * @param output è¾“å‡ºå‘é‡ï¼ŒåŒ…å« 1200x1200 çš„ confidence matrix
     */
    void copyOutputFromDevice(std::vector<float>& output);
    
    // TensorRT æˆå‘˜å˜é‡
    Logger logger_;
    std::unique_ptr<nvinfer1::IRuntime> runtime_;
    std::unique_ptr<nvinfer1::ICudaEngine> engine_;
    std::unique_ptr<nvinfer1::IExecutionContext> context_;
    
    // CUDA ç›¸å…³
    cudaStream_t stream_;
    void* input_buffer0_;
    void* input_buffer1_;
    void* output_buffer_;
    
    // ç¼“å†²åŒºå¤§å°
    size_t input_size_;
    size_t output_size_;
    
    // ç»‘å®šä¿¡æ¯
    std::vector<void*> bindings_;
    std::vector<int> input_indices_;
    std::vector<int> output_indices_;
    
#else
    // ğŸ”§ é TensorRT ç¯å¢ƒä¸‹çš„å ä½ç¬¦æˆå‘˜
    Logger logger_;  // ä»ç„¶éœ€è¦è¿™ä¸ªæˆå‘˜ï¼Œä½†ä¼šä½¿ç”¨ç©ºå®ç°
#endif // TENSORRT_AVAILABLE
};

#endif // LOFTR_TENSORRT_H
